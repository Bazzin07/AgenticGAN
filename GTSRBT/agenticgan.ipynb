{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2877a40",
   "metadata": {},
   "source": [
    "# AgenticGAN Tester: GTSRB (German Traffic Sign Recognition)\n",
    "\n",
    "##  Project Overview\n",
    "AgenticGAN-Tester is a novel AI framework that autonomously discovers and generates realistic failure cases for deep learning models using an agent-driven GAN approach.\n",
    "\n",
    "**Dataset:** GTSRB - German Traffic Sign Recognition Benchmark\n",
    "- **43 Classes** of traffic signs\n",
    "- **RGB Images** (variable sizes, resized to 32x32)\n",
    "- **~39,000 training images**, **~12,600 test images**\n",
    "\n",
    "##  Pipeline Phases:\n",
    "1. **Phase I:** Train Base Classifier (CNN) on GTSRB\n",
    "2. **Phase II:** Train Conditional GAN to generate traffic signs\n",
    "3. **Phase III:** Agent mines failure cases (low-confidence GAN outputs)\n",
    "4. **Phase IV:** Retrain classifier with real + failure images (Hardening)\n",
    "5. **Phase V:** Evaluate accuracy improvement + Grad-CAM visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4293003",
   "metadata": {},
   "source": [
    "##  Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: mps\n",
      "‚úÖ Configuration loaded successfully!\n",
      "üìä Dataset: GTSRB (43 classes)\n",
      "üìÅ Models will be saved to: /Users/ayushgourav/projectss/AgenticGAN---Tester-/GTSRBT/AGENT/models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  CONFIGURATION & IMPORTS\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "\n",
    "#  GLOBAL CONFIG (Change these as needed)\n",
    "\n",
    "CONFIG = {\n",
    "    # Dataset\n",
    "    \"num_classes\": 43,          # GTSRB has 43 traffic sign classes\n",
    "    \"img_size\": 32,\n",
    "    \"channels\": 3,\n",
    "    \n",
    "    # Paths (relative to this notebook)\n",
    "    \"data_root\": \"../DATA/archive\",\n",
    "    \"train_csv\": \"../DATA/archive/Train.csv\",\n",
    "    \"test_csv\": \"../DATA/archive/Test.csv\",\n",
    "    \"train_images\": \"../DATA/archive/Train\",\n",
    "    \"test_images\": \"../DATA/archive/Test\",\n",
    "\n",
    "    # Model paths\n",
    "    \"model_dir\": \"./models\",\n",
    "    \"classifier_path\": \"./models/classifier.pth\",\n",
    "    \"generator_path\": \"./models/gan_generator.pth\",\n",
    "    \"discriminator_path\": \"./models/gan_discriminator.pth\",\n",
    "    \"hardened_path\": \"./models/classifier_hardened.pth\",\n",
    "    \n",
    "    # Output paths\n",
    "    \"gen_images_dir\": \"./generated_images\",\n",
    "    \"failure_dir\": \"./failure_cases\",\n",
    "    \"retrain_data_dir\": \"./retrain_data\",\n",
    "    \"results_dir\": \"./results\",\n",
    "    \n",
    "    # Training Hyperparameters\n",
    "    \"classifier_epochs\": 10,\n",
    "    \"classifier_lr\": 1e-3,\n",
    "    \"classifier_batch_size\": 64,\n",
    "    \n",
    "    \"gan_epochs\": 100,\n",
    "    \"gan_lr\": 2e-4,\n",
    "    \"gan_batch_size\": 128,\n",
    "    \"latent_dim\": 128,\n",
    "    \n",
    "    \"retrain_epochs\": 5,\n",
    "    \"retrain_lr\": 1e-5,          # Very small to prevent forgetting\n",
    "    \"num_real_samples\": 10000,   # Real images for retraining\n",
    "    \n",
    "    # Failure Mining\n",
    "    \"num_gan_samples\": 2000,\n",
    "    \"confidence_threshold\": 0.90,\n",
    "    \n",
    "    \n",
    "    \"num_workers\": 0,           \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "# DEVICE SETUP\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "\n",
    "#  CREATE DIRECTORIES\n",
    "\n",
    "for key in [\"model_dir\", \"gen_images_dir\", \"failure_dir\", \"retrain_data_dir\", \"results_dir\"]:\n",
    "    os.makedirs(CONFIG[key], exist_ok=True)\n",
    "\n",
    "print(\" Configuration loaded successfully!\")\n",
    "print(f\" Dataset: GTSRB ({CONFIG['num_classes']} classes)\")\n",
    "print(f\" Models will be saved to: {os.path.abspath(CONFIG['model_dir'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f45463",
   "metadata": {},
   "source": [
    "## Model Architectures (Classifier + GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22063d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model architectures defined:\n",
      "   - RobustCNN (Classifier)\n",
      "   - Generator (Conditional GAN)\n",
      "   - Discriminator (PatchGAN)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  MODEL ARCHITECTURES\n",
    "\n",
    "\n",
    "\n",
    "#  Robust CNN Classifier\n",
    "\n",
    "class RobustCNN(nn.Module):\n",
    "    def __init__(self, num_classes=43):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(8 * 8 * 128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "#  Conditional Generator (Class-Aware)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_classes=43, img_size=32):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.init_size = img_size // 4\n",
    "\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, \n",
    "                            256 * self.init_size * self.init_size)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embed = self.label_emb(labels)\n",
    "        x = torch.cat((z, label_embed), dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 256, self.init_size, self.init_size)\n",
    "        return self.net(x)\n",
    "\n",
    "#  PatchGAN Discriminator (Spectral Norm)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        def block(in_c, out_c, normalize=True):\n",
    "            layers = [spectral_norm(nn.Conv2d(in_c, out_c, 4, 2, 1))]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_c))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(3, 64, normalize=False),\n",
    "            *block(64, 128),\n",
    "            *block(128, 256),\n",
    "            spectral_norm(nn.Conv2d(256, 1, 4, 1, 0))\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "print(\" Model architectures defined:\")\n",
    "print(\"   - RobustCNN (Classifier)\")\n",
    "print(\"   - Generator (Conditional GAN)\")\n",
    "print(\"   - Discriminator (PatchGAN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e8e32",
   "metadata": {},
   "source": [
    "##  Custom Dataset Classes (Kaggle GTSRB Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e98cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom dataset classes defined\n",
      "   - GTSRBDataset (CSV-based)\n",
      "   - GTSRBTestDataset (alias)\n",
      "   - MixedDataset (for retraining)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  CUSTOM DATASET CLASSES\n",
    "\n",
    "\n",
    "# Standard transform for all phases\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "#  Unified GTSRB Dataset (CSV-based for CORRECT labels)\n",
    "\n",
    "class GTSRBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Works for BOTH Train.csv and Test.csv\n",
    "    Ensures ClassId in CSV matches the label used by model\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.csv_file = csv_file\n",
    "        \n",
    "        # Detect column names\n",
    "        self.path_col = 'Path' if 'Path' in self.df.columns else 'image_path'\n",
    "        self.label_col = 'ClassId' if 'ClassId' in self.df.columns else 'label'\n",
    "        \n",
    "        print(f\"   Loaded {len(self.df)} samples from {os.path.basename(csv_file)}\")\n",
    "        print(f\"   Label range: {self.df[self.label_col].min()} to {self.df[self.label_col].max()}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.df.iloc[idx][self.path_col]\n",
    "        full_img_path = os.path.join(self.root_dir, rel_path)\n",
    "        \n",
    "        if not os.path.exists(full_img_path):\n",
    "            basename = os.path.basename(rel_path)\n",
    "            parent = os.path.dirname(rel_path)\n",
    "            if \"Train\" in self.csv_file:\n",
    "                full_img_path = os.path.join(self.root_dir, \"Train\", parent, basename)\n",
    "            else:\n",
    "                full_img_path = os.path.join(self.root_dir, \"Test\", basename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(full_img_path).convert('RGB')\n",
    "        except (FileNotFoundError, OSError):\n",
    "            image = Image.new('RGB', (32, 32), color='black')\n",
    "\n",
    "        label = int(self.df.iloc[idx][self.label_col])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Alias for backward compatibility\n",
    "GTSRBTestDataset = GTSRBDataset\n",
    "\n",
    "\n",
    "#  Mixed Dataset (Real + Failure Images)\n",
    "\n",
    "class MixedDataset(Dataset):\n",
    "    def __init__(self, real_dir, fail_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        if os.path.exists(real_dir):\n",
    "            for fname in os.listdir(real_dir):\n",
    "                if fname.endswith(\".png\") and \"label_\" in fname:\n",
    "                    try:\n",
    "                        label = int(fname.split(\"label_\")[1].split(\".\")[0])\n",
    "                        self.samples.append((os.path.join(real_dir, fname), label))\n",
    "                    except: \n",
    "                        pass\n",
    "\n",
    "        if os.path.exists(fail_dir):\n",
    "            for fname in os.listdir(fail_dir):\n",
    "                if fname.endswith(\".png\") and \"label_\" in fname:\n",
    "                    try:\n",
    "                        label = int(fname.split(\"label_\")[1].split(\".\")[0])\n",
    "                        self.samples.append((os.path.join(fail_dir, fname), label))\n",
    "                    except: \n",
    "                        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "print(\" Custom dataset classes defined\")\n",
    "print(\"   - GTSRBDataset (CSV-based)\")\n",
    "print(\"   - GTSRBTestDataset (alias)\")\n",
    "print(\"   - MixedDataset (for retraining)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df5c21",
   "metadata": {},
   "source": [
    "\n",
    "#  Phase I: Train Base Classifier\n",
    "\n",
    "Train a CNN classifier on the GTSRB training set to recognize 43 traffic sign classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbba1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading training data from CSV...\n",
      "   Loaded 39209 samples from Train.csv\n",
      "   Label range: 0 to 42\n",
      "üìä Loading test data from CSV...\n",
      "   Loaded 12630 samples from Test.csv\n",
      "   Label range: 0 to 42\n",
      "\n",
      "üöÄ Starting classifier training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/613 [00:00<?, ?it/s]/Users/ayushgourav/projectss/AgenticGAN---Tester-/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1/10:   0%|          | 0/613 [00:00<?, ?it/s, loss=3.8046]/Users/ayushgourav/projectss/AgenticGAN---Tester-/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:36<00:00, 16.90it/s, loss=0.0538]\n",
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:36<00:00, 16.90it/s, loss=0.0538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Loss: 434.7373 | Train Acc: 79.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:56<00:00, 10.82it/s, loss=0.2035]\n",
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:56<00:00, 10.82it/s, loss=0.2035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] | Loss: 30.5098 | Train Acc: 98.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:59<00:00, 10.26it/s, loss=0.0267]\n",
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:59<00:00, 10.26it/s, loss=0.0267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] | Loss: 23.1607 | Train Acc: 98.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:59<00:00, 10.32it/s, loss=0.0681]\n",
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:59<00:00, 10.32it/s, loss=0.0681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] | Loss: 15.8329 | Train Acc: 99.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:57<00:00, 10.63it/s, loss=0.0168]\n",
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:57<00:00, 10.63it/s, loss=0.0168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] | Loss: 17.3650 | Train Acc: 99.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:58<00:00, 10.52it/s, loss=0.0003]\n",
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:58<00:00, 10.52it/s, loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] | Loss: 14.2450 | Train Acc: 99.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:29<00:00, 20.84it/s, loss=0.0079]\n",
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:29<00:00, 20.84it/s, loss=0.0079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] | Loss: 11.6080 | Train Acc: 99.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:28<00:00, 21.43it/s, loss=0.0055]\n",
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:28<00:00, 21.43it/s, loss=0.0055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] | Loss: 10.0064 | Train Acc: 99.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:28<00:00, 21.75it/s, loss=0.0081]\n",
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:28<00:00, 21.75it/s, loss=0.0081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] | Loss: 10.5830 | Train Acc: 99.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 613/613 [00:28<00:00, 21.24it/s, loss=0.0901]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] | Loss: 10.3530 | Train Acc: 99.48%\n",
      "\n",
      "‚úÖ Base Classifier saved to: /Users/ayushgourav/projectss/AgenticGAN---Tester-/GTSRBT/AGENT/models/classifier.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:04<00:00, 41.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Base Classifier Test Accuracy: 96.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  PHASE I: TRAIN BASE CLASSIFIER\n",
    "\n",
    "\n",
    "\n",
    "#  Load Training Data (CSV-based for CORRECT labels)\n",
    "\n",
    "print(\"üìä Loading training data from CSV...\")\n",
    "train_data = GTSRBDataset(\n",
    "    csv_file=CONFIG[\"train_csv\"],\n",
    "    root_dir=CONFIG[\"data_root\"],\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=CONFIG[\"classifier_batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "#  Load Test Data (CSV-based)\n",
    "\n",
    "print(\"üìä Loading test data from CSV...\")\n",
    "test_data = GTSRBDataset(\n",
    "    csv_file=CONFIG[\"test_csv\"],\n",
    "    root_dir=CONFIG[\"data_root\"],\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=CONFIG[\"classifier_batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "#  Initialize Model\n",
    "\n",
    "classifier = RobustCNN(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=CONFIG[\"classifier_lr\"])\n",
    "\n",
    "\n",
    "#  Training Loop\n",
    "\n",
    "def train_classifier(model, loader, epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for images, labels in loop:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        acc = 100.0 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {running_loss:.4f} | Train Acc: {acc:.2f}%\")\n",
    "\n",
    "\n",
    "# üîπ Train & Save\n",
    "\n",
    "print(\"\\n Starting classifier training...\")\n",
    "train_classifier(classifier, train_loader, CONFIG[\"classifier_epochs\"])\n",
    "\n",
    "torch.save(classifier.state_dict(), CONFIG[\"classifier_path\"])\n",
    "print(f\"\\n Base Classifier saved to: {os.path.abspath(CONFIG['classifier_path'])}\")\n",
    "\n",
    "\n",
    "#  Quick Evaluation\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "classifier.eval()\n",
    "base_accuracy = evaluate_model(classifier, test_loader)\n",
    "print(f\"\\n Base Classifier Test Accuracy: {base_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bf067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:04<00:00, 47.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Base Classifier Test Accuracy: 96.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate Base Classifier on Test Set\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "# Evaluate\n",
    "classifier.eval()\n",
    "base_accuracy = evaluate_model(classifier, test_loader)\n",
    "print(f\"\\n Base Classifier Test Accuracy: {base_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b877c15",
   "metadata": {},
   "source": [
    "\n",
    "#  Phase II: Train Conditional GAN\n",
    "\n",
    "Train a class-conditional GAN to generate realistic traffic sign images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a7b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading GAN training data from CSV...\n",
      "   Loaded 39209 samples from Train.csv\n",
      "   Label range: 0 to 42\n",
      "\n",
      "üé® Starting GAN training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 1/100:   0%|          | 0/307 [00:00<?, ?it/s]/Users/ayushgourav/projectss/AgenticGAN---Tester-/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ayushgourav/projectss/AgenticGAN---Tester-/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "GAN Epoch 1/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:47<00:00,  6.46it/s, D_loss=1.4899, G_loss=0.8354] \n",
      "GAN Epoch 1/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:47<00:00,  6.46it/s, D_loss=1.4899, G_loss=0.8354]\n",
      "GAN Epoch 2/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:47<00:00,  6.50it/s, D_loss=1.3026, G_loss=0.3376] \n",
      "GAN Epoch 2/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:47<00:00,  6.50it/s, D_loss=1.3026, G_loss=0.3376]\n",
      "GAN Epoch 3/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:47<00:00,  6.47it/s, D_loss=1.1384, G_loss=0.4345] \n",
      "GAN Epoch 3/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:47<00:00,  6.47it/s, D_loss=1.1384, G_loss=0.4345]\n",
      "GAN Epoch 4/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=1.3432, G_loss=0.5922] \n",
      "GAN Epoch 4/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=1.3432, G_loss=0.5922]\n",
      "GAN Epoch 5/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.13it/s, D_loss=1.3541, G_loss=0.3029] \n",
      "GAN Epoch 5/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.13it/s, D_loss=1.3541, G_loss=0.3029]\n",
      "GAN Epoch 6/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.16it/s, D_loss=1.1879, G_loss=0.3111] \n",
      "GAN Epoch 6/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.16it/s, D_loss=1.1879, G_loss=0.3111]\n",
      "GAN Epoch 7/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.15it/s, D_loss=1.1610, G_loss=0.9459] \n",
      "GAN Epoch 7/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.15it/s, D_loss=1.1610, G_loss=0.9459]\n",
      "GAN Epoch 8/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.24it/s, D_loss=0.6836, G_loss=0.0332] \n",
      "GAN Epoch 8/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.24it/s, D_loss=0.6836, G_loss=0.0332]\n",
      "GAN Epoch 9/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.34it/s, D_loss=1.5339, G_loss=0.4411] \n",
      "GAN Epoch 9/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.34it/s, D_loss=1.5339, G_loss=0.4411]\n",
      "GAN Epoch 10/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.35it/s, D_loss=0.9750, G_loss=0.5131] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 11/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.32it/s, D_loss=1.0653, G_loss=1.8353] \n",
      "GAN Epoch 11/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.32it/s, D_loss=1.0653, G_loss=1.8353]\n",
      "GAN Epoch 12/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.19it/s, D_loss=1.1401, G_loss=1.6909] \n",
      "GAN Epoch 12/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.19it/s, D_loss=1.1401, G_loss=1.6909] \n",
      "GAN Epoch 13/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.11it/s, D_loss=0.7184, G_loss=0.8670] \n",
      "GAN Epoch 13/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.11it/s, D_loss=0.7184, G_loss=0.8670]\n",
      "GAN Epoch 14/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.19it/s, D_loss=1.0814, G_loss=1.1300] \n",
      "GAN Epoch 14/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.19it/s, D_loss=1.0814, G_loss=1.1300]\n",
      "GAN Epoch 15/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.30it/s, D_loss=0.9763, G_loss=1.2192] \n",
      "GAN Epoch 15/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.30it/s, D_loss=0.9763, G_loss=1.2192]\n",
      "GAN Epoch 16/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=1.0676, G_loss=0.9305] \n",
      "GAN Epoch 16/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=1.0676, G_loss=0.9305]\n",
      "GAN Epoch 17/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.23it/s, D_loss=0.7952, G_loss=1.6786] \n",
      "GAN Epoch 17/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.23it/s, D_loss=0.7952, G_loss=1.6786]\n",
      "GAN Epoch 18/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.10it/s, D_loss=1.0718, G_loss=1.4614] \n",
      "GAN Epoch 18/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.10it/s, D_loss=1.0718, G_loss=1.4614]\n",
      "GAN Epoch 19/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.04it/s, D_loss=0.8956, G_loss=0.8905] \n",
      "GAN Epoch 19/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.04it/s, D_loss=0.8956, G_loss=0.8905]\n",
      "GAN Epoch 20/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.13it/s, D_loss=1.1072, G_loss=0.4469] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_20.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 21/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=1.0020, G_loss=1.2750] \n",
      "GAN Epoch 21/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=1.0020, G_loss=1.2750]\n",
      "GAN Epoch 22/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=1.2440, G_loss=-0.1912]\n",
      "GAN Epoch 22/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=1.2440, G_loss=-0.1912]\n",
      "GAN Epoch 23/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.17it/s, D_loss=0.7683, G_loss=1.8100] \n",
      "GAN Epoch 23/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.17it/s, D_loss=0.7683, G_loss=1.8100]\n",
      "GAN Epoch 24/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.12it/s, D_loss=0.8859, G_loss=1.3643] \n",
      "GAN Epoch 24/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.12it/s, D_loss=0.8859, G_loss=1.3643]\n",
      "GAN Epoch 25/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.15it/s, D_loss=1.0300, G_loss=1.6397] \n",
      "GAN Epoch 25/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.15it/s, D_loss=1.0300, G_loss=1.6397]\n",
      "GAN Epoch 26/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.23it/s, D_loss=0.8073, G_loss=1.7246] \n",
      "GAN Epoch 26/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.23it/s, D_loss=0.8073, G_loss=1.7246]\n",
      "GAN Epoch 27/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=0.8501, G_loss=1.7345] \n",
      "GAN Epoch 27/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=0.8501, G_loss=1.7345]\n",
      "GAN Epoch 28/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.32it/s, D_loss=0.8610, G_loss=0.8351] \n",
      "GAN Epoch 28/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.32it/s, D_loss=0.8610, G_loss=0.8351]\n",
      "GAN Epoch 29/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=0.9253, G_loss=1.6570] \n",
      "GAN Epoch 29/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=0.9253, G_loss=1.6570]\n",
      "GAN Epoch 30/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=0.5907, G_loss=1.7824] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_30.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 31/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.12it/s, D_loss=0.9168, G_loss=1.6750] \n",
      "GAN Epoch 31/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.12it/s, D_loss=0.9168, G_loss=1.6750]\n",
      "GAN Epoch 32/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=0.7017, G_loss=0.3691] \n",
      "GAN Epoch 32/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=0.7017, G_loss=0.3691]\n",
      "GAN Epoch 33/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.32it/s, D_loss=0.4301, G_loss=1.4245] \n",
      "GAN Epoch 33/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.32it/s, D_loss=0.4301, G_loss=1.4245]\n",
      "GAN Epoch 34/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.36it/s, D_loss=1.0841, G_loss=1.1116] \n",
      "GAN Epoch 34/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.36it/s, D_loss=1.0841, G_loss=1.1116]\n",
      "GAN Epoch 35/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.38it/s, D_loss=0.6146, G_loss=0.2069]\n",
      "GAN Epoch 35/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.38it/s, D_loss=0.6146, G_loss=0.2069]\n",
      "GAN Epoch 36/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.29it/s, D_loss=0.9070, G_loss=0.3856] \n",
      "GAN Epoch 36/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.29it/s, D_loss=0.9070, G_loss=0.3856]\n",
      "GAN Epoch 37/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.05it/s, D_loss=0.7774, G_loss=0.6041] \n",
      "GAN Epoch 37/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.05it/s, D_loss=0.7774, G_loss=0.6041]\n",
      "GAN Epoch 38/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.11it/s, D_loss=0.6425, G_loss=0.6221] \n",
      "GAN Epoch 38/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.11it/s, D_loss=0.6425, G_loss=0.6221]\n",
      "GAN Epoch 39/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.22it/s, D_loss=0.6841, G_loss=1.5107] \n",
      "GAN Epoch 39/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.22it/s, D_loss=0.6841, G_loss=1.5107]\n",
      "GAN Epoch 40/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.27it/s, D_loss=1.1871, G_loss=1.5642] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_40.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 41/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.28it/s, D_loss=1.0695, G_loss=0.1637] \n",
      "GAN Epoch 41/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.28it/s, D_loss=1.0695, G_loss=0.1637]\n",
      "GAN Epoch 42/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=1.2158, G_loss=1.9439] \n",
      "GAN Epoch 42/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.20it/s, D_loss=1.2158, G_loss=1.9439]\n",
      "GAN Epoch 43/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=0.7073, G_loss=1.0268]\n",
      "GAN Epoch 43/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=0.7073, G_loss=1.0268]\n",
      "GAN Epoch 44/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.09it/s, D_loss=0.8393, G_loss=0.7561] \n",
      "GAN Epoch 44/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.09it/s, D_loss=0.8393, G_loss=0.7561]\n",
      "GAN Epoch 45/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.23it/s, D_loss=0.6382, G_loss=1.2457] \n",
      "GAN Epoch 45/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.23it/s, D_loss=0.6382, G_loss=1.2457]\n",
      "GAN Epoch 46/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.28it/s, D_loss=0.6207, G_loss=1.3866] \n",
      "GAN Epoch 46/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.28it/s, D_loss=0.6207, G_loss=1.3866]\n",
      "GAN Epoch 47/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.28it/s, D_loss=0.8529, G_loss=0.3991] \n",
      "GAN Epoch 47/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.28it/s, D_loss=0.8529, G_loss=0.3991]\n",
      "GAN Epoch 48/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.15it/s, D_loss=1.2338, G_loss=0.4472]\n",
      "GAN Epoch 48/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.15it/s, D_loss=1.2338, G_loss=0.4472]\n",
      "GAN Epoch 49/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.09it/s, D_loss=1.0922, G_loss=1.6735]\n",
      "GAN Epoch 49/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.09it/s, D_loss=1.0922, G_loss=1.6735]\n",
      "GAN Epoch 50/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.10it/s, D_loss=0.6438, G_loss=1.2963] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_50.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 51/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.32it/s, D_loss=0.5934, G_loss=1.9553] \n",
      "GAN Epoch 51/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.32it/s, D_loss=0.5934, G_loss=1.9553]\n",
      "GAN Epoch 52/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.35it/s, D_loss=0.2921, G_loss=0.9626] \n",
      "GAN Epoch 52/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.35it/s, D_loss=0.2921, G_loss=0.9626]\n",
      "GAN Epoch 53/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=0.8253, G_loss=0.4595]\n",
      "GAN Epoch 53/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=0.8253, G_loss=0.4595]\n",
      "GAN Epoch 54/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.28it/s, D_loss=0.6981, G_loss=1.7643] \n",
      "GAN Epoch 54/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.28it/s, D_loss=0.6981, G_loss=1.7643]\n",
      "GAN Epoch 55/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.08it/s, D_loss=0.4176, G_loss=0.5883] \n",
      "GAN Epoch 55/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.08it/s, D_loss=0.4176, G_loss=0.5883]\n",
      "GAN Epoch 56/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.10it/s, D_loss=0.9847, G_loss=0.2676] \n",
      "GAN Epoch 56/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.10it/s, D_loss=0.9847, G_loss=0.2676]\n",
      "GAN Epoch 57/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.25it/s, D_loss=0.8909, G_loss=0.4946] \n",
      "GAN Epoch 57/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.25it/s, D_loss=0.8909, G_loss=0.4946]\n",
      "GAN Epoch 58/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.33it/s, D_loss=0.4175, G_loss=1.0957] \n",
      "GAN Epoch 58/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.33it/s, D_loss=0.4175, G_loss=1.0957]\n",
      "GAN Epoch 59/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.33it/s, D_loss=0.7736, G_loss=2.2109] \n",
      "GAN Epoch 59/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.33it/s, D_loss=0.7736, G_loss=2.2109]\n",
      "GAN Epoch 60/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.31it/s, D_loss=0.2921, G_loss=0.4660] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_60.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 61/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.14it/s, D_loss=1.1428, G_loss=1.2018]\n",
      "GAN Epoch 61/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.14it/s, D_loss=1.1428, G_loss=1.2018]\n",
      "GAN Epoch 62/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.11it/s, D_loss=0.8451, G_loss=0.3196] \n",
      "GAN Epoch 62/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.11it/s, D_loss=0.8451, G_loss=0.3196]\n",
      "GAN Epoch 63/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.26it/s, D_loss=1.0550, G_loss=0.3333] \n",
      "GAN Epoch 63/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.26it/s, D_loss=1.0550, G_loss=0.3333]\n",
      "GAN Epoch 64/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.34it/s, D_loss=0.8259, G_loss=0.4054] \n",
      "GAN Epoch 64/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.34it/s, D_loss=0.8259, G_loss=0.4054]\n",
      "GAN Epoch 65/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.36it/s, D_loss=0.5558, G_loss=0.7292]\n",
      "GAN Epoch 65/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.36it/s, D_loss=0.5558, G_loss=0.7292]\n",
      "GAN Epoch 66/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.33it/s, D_loss=1.4188, G_loss=0.8665]\n",
      "GAN Epoch 66/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:48<00:00,  6.33it/s, D_loss=1.4188, G_loss=0.8665]\n",
      "GAN Epoch 67/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.17it/s, D_loss=0.6075, G_loss=0.7418]\n",
      "GAN Epoch 67/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.17it/s, D_loss=0.6075, G_loss=0.7418]\n",
      "GAN Epoch 68/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.13it/s, D_loss=0.7175, G_loss=1.5162]\n",
      "GAN Epoch 68/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.13it/s, D_loss=0.7175, G_loss=1.5162]\n",
      "GAN Epoch 69/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.24it/s, D_loss=0.9353, G_loss=0.9314]\n",
      "GAN Epoch 69/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.24it/s, D_loss=0.9353, G_loss=0.9314]\n",
      "GAN Epoch 70/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.24it/s, D_loss=1.1413, G_loss=1.7252] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_70.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 71/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.25it/s, D_loss=1.0628, G_loss=0.2844]\n",
      "GAN Epoch 71/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.25it/s, D_loss=1.0628, G_loss=0.2844]\n",
      "GAN Epoch 72/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.22it/s, D_loss=0.6122, G_loss=0.9627]\n",
      "GAN Epoch 72/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.22it/s, D_loss=0.6122, G_loss=0.9627]\n",
      "GAN Epoch 73/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.07it/s, D_loss=0.2631, G_loss=0.8215]\n",
      "GAN Epoch 73/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.07it/s, D_loss=0.2631, G_loss=0.8215]\n",
      "GAN Epoch 74/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.08it/s, D_loss=0.7964, G_loss=0.0383]\n",
      "GAN Epoch 74/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.08it/s, D_loss=0.7964, G_loss=0.0383]\n",
      "GAN Epoch 75/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=1.1357, G_loss=0.3336]\n",
      "GAN Epoch 75/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=1.1357, G_loss=0.3336]\n",
      "GAN Epoch 76/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.21it/s, D_loss=0.9363, G_loss=-0.0021]\n",
      "GAN Epoch 76/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.21it/s, D_loss=0.9363, G_loss=-0.0021]\n",
      "GAN Epoch 77/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.22it/s, D_loss=0.3234, G_loss=2.2707] \n",
      "GAN Epoch 77/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.22it/s, D_loss=0.3234, G_loss=2.2707]\n",
      "GAN Epoch 78/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.17it/s, D_loss=0.4655, G_loss=1.6369] \n",
      "GAN Epoch 78/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.17it/s, D_loss=0.4655, G_loss=1.6369]\n",
      "GAN Epoch 79/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.10it/s, D_loss=1.1420, G_loss=0.6421] \n",
      "GAN Epoch 79/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.10it/s, D_loss=1.1420, G_loss=0.6421]\n",
      "GAN Epoch 80/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.10it/s, D_loss=1.7755, G_loss=0.8609] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_80.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 81/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=1.2064, G_loss=0.2840]\n",
      "GAN Epoch 81/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=1.2064, G_loss=0.2840]\n",
      "GAN Epoch 82/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.16it/s, D_loss=0.7403, G_loss=1.7213] \n",
      "GAN Epoch 82/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.16it/s, D_loss=0.7403, G_loss=1.7213]\n",
      "GAN Epoch 83/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=0.5811, G_loss=1.9633]\n",
      "GAN Epoch 83/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.14it/s, D_loss=0.5811, G_loss=1.9633]\n",
      "GAN Epoch 84/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.14it/s, D_loss=0.9893, G_loss=0.5978] \n",
      "GAN Epoch 84/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.14it/s, D_loss=0.9893, G_loss=0.5978]\n",
      "GAN Epoch 85/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.07it/s, D_loss=0.7842, G_loss=0.9461] \n",
      "GAN Epoch 85/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.07it/s, D_loss=0.7842, G_loss=0.9461]\n",
      "GAN Epoch 86/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.12it/s, D_loss=0.5058, G_loss=1.7304]\n",
      "GAN Epoch 86/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:50<00:00,  6.12it/s, D_loss=0.5058, G_loss=1.7304]\n",
      "GAN Epoch 87/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.16it/s, D_loss=1.2265, G_loss=-0.1408]\n",
      "GAN Epoch 87/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.16it/s, D_loss=1.2265, G_loss=-0.1408]\n",
      "GAN Epoch 88/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.21it/s, D_loss=0.7805, G_loss=1.7326]\n",
      "GAN Epoch 88/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.21it/s, D_loss=0.7805, G_loss=1.7326]\n",
      "GAN Epoch 89/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.17it/s, D_loss=0.6033, G_loss=1.5606]\n",
      "GAN Epoch 89/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:49<00:00,  6.17it/s, D_loss=0.6033, G_loss=1.5606]\n",
      "GAN Epoch 90/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:53<00:00,  5.69it/s, D_loss=0.7755, G_loss=0.6289] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_90.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GAN Epoch 91/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:56<00:00,  5.44it/s, D_loss=1.0159, G_loss=1.3691]\n",
      "GAN Epoch 91/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:56<00:00,  5.44it/s, D_loss=1.0159, G_loss=1.3691]\n",
      "GAN Epoch 92/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:56<00:00,  5.45it/s, D_loss=1.0595, G_loss=1.5669] \n",
      "GAN Epoch 92/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:56<00:00,  5.45it/s, D_loss=1.0595, G_loss=1.5669]\n",
      "GAN Epoch 93/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:55<00:00,  5.50it/s, D_loss=1.0396, G_loss=1.0848] \n",
      "GAN Epoch 93/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:55<00:00,  5.50it/s, D_loss=1.0396, G_loss=1.0848]\n",
      "GAN Epoch 94/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:55<00:00,  5.52it/s, D_loss=0.8776, G_loss=1.2236]\n",
      "GAN Epoch 94/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:55<00:00,  5.52it/s, D_loss=0.8776, G_loss=1.2236]\n",
      "GAN Epoch 95/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:56<00:00,  5.47it/s, D_loss=0.4559, G_loss=0.5926]\n",
      "GAN Epoch 95/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:56<00:00,  5.47it/s, D_loss=0.4559, G_loss=0.5926]\n",
      "GAN Epoch 96/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:57<00:00,  5.36it/s, D_loss=0.8863, G_loss=1.6461]\n",
      "GAN Epoch 96/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:57<00:00,  5.36it/s, D_loss=0.8863, G_loss=1.6461]\n",
      "GAN Epoch 97/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:57<00:00,  5.38it/s, D_loss=0.1549, G_loss=2.4382]\n",
      "GAN Epoch 97/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:57<00:00,  5.38it/s, D_loss=0.1549, G_loss=2.4382]\n",
      "GAN Epoch 98/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:56<00:00,  5.41it/s, D_loss=1.2477, G_loss=1.9451]\n",
      "GAN Epoch 98/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:56<00:00,  5.41it/s, D_loss=1.2477, G_loss=1.9451]\n",
      "GAN Epoch 99/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:58<00:00,  5.21it/s, D_loss=0.4769, G_loss=1.1094]\n",
      "GAN Epoch 99/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:58<00:00,  5.21it/s, D_loss=0.4769, G_loss=1.1094]\n",
      "GAN Epoch 100/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [01:04<00:00,  4.77it/s, D_loss=0.9551, G_loss=1.6993]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Saved samples: ./generated_images/epoch_100.png\n",
      "\n",
      "‚úÖ GAN models saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  PHASE II: TRAIN CONDITIONAL GAN\n",
    "\n",
    "\n",
    "#  GAN Data Loader (CSV-based for correct labels)\n",
    "\n",
    "print(\"Loading GAN training data from CSV...\")\n",
    "gan_dataset = GTSRBDataset(\n",
    "    csv_file=CONFIG[\"train_csv\"],\n",
    "    root_dir=CONFIG[\"data_root\"],\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "gan_loader = DataLoader(\n",
    "    gan_dataset,\n",
    "    batch_size=CONFIG[\"gan_batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize GAN Models\n",
    "\n",
    "generator = Generator(\n",
    "    latent_dim=CONFIG[\"latent_dim\"],\n",
    "    num_classes=CONFIG[\"num_classes\"]\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "\n",
    "#  Hinge Loss Functions\n",
    "\n",
    "def d_hinge_loss(real_logits, fake_logits):\n",
    "    return torch.mean(F.relu(1.0 - real_logits)) + torch.mean(F.relu(1.0 + fake_logits))\n",
    "\n",
    "def g_hinge_loss(fake_logits):\n",
    "    return -torch.mean(fake_logits)\n",
    "\n",
    "\n",
    "#  Optimizers (TTUR: different learning rates)\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=CONFIG[\"gan_lr\"], betas=(0.0, 0.9))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=CONFIG[\"gan_lr\"], betas=(0.0, 0.9))\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "print(\"\\nStarting GAN training...\")\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(CONFIG[\"gan_epochs\"]):\n",
    "    loop = tqdm(gan_loader, desc=f\"GAN Epoch {epoch+1}/{CONFIG['gan_epochs']}\")\n",
    "    \n",
    "    for real_imgs, labels in loop:\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        z = torch.randn(batch_size, CONFIG[\"latent_dim\"], device=device)\n",
    "        fake_imgs = generator(z, labels).detach()\n",
    "        \n",
    "        real_logits = discriminator(real_imgs)\n",
    "        fake_logits = discriminator(fake_imgs)\n",
    "        \n",
    "        loss_D = d_hinge_loss(real_logits, fake_logits)\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        #  Train Generator \n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        z = torch.randn(batch_size, CONFIG[\"latent_dim\"], device=device)\n",
    "        fake_imgs = generator(z, labels)\n",
    "        fake_logits = discriminator(fake_imgs)\n",
    "        \n",
    "        loss_G = g_hinge_loss(fake_logits)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        loop.set_postfix(D_loss=f\"{loss_D.item():.4f}\", G_loss=f\"{loss_G.item():.4f}\")\n",
    "\n",
    "    # Save sample images every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        z = torch.randn(25, CONFIG[\"latent_dim\"], device=device)\n",
    "        sample_labels = torch.randint(0, CONFIG[\"num_classes\"], (25,), device=device)\n",
    "        samples = generator(z, sample_labels)\n",
    "        \n",
    "        save_path = os.path.join(CONFIG[\"gen_images_dir\"], f\"epoch_{epoch+1}.png\")\n",
    "        save_image(samples, save_path, nrow=5, normalize=True)\n",
    "        print(f\"üñºÔ∏è Saved samples: {save_path}\")\n",
    "\n",
    "\n",
    "# üîπ Save GAN Models\n",
    "\n",
    "torch.save(generator.state_dict(), CONFIG[\"generator_path\"])\n",
    "torch.save(discriminator.state_dict(), CONFIG[\"discriminator_path\"])\n",
    "print(f\"\\nGAN models saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c33bf7",
   "metadata": {},
   "source": [
    "\n",
    "#  Phase III: Agentic Failure Mining\n",
    "\n",
    "Generate GAN images and identify cases where the classifier has low confidence (potential failure cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbf19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models loaded for failure mining\n",
      "\n",
      "üîç Mining 2000 GAN samples for failures...\n",
      "   Confidence threshold: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mining: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:28<00:00, 69.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Failure Mining Complete!\n",
      "   Total failures found: 699\n",
      "   Saved to: /Users/ayushgourav/projectss/AgenticGAN---Tester-/GTSRBT/AGENT/failure_cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PHASE III: AGENTIC FAILURE MINING\n",
    "\n",
    "\n",
    "# Clear old failure cases\n",
    "if os.path.exists(CONFIG[\"failure_dir\"]):\n",
    "    shutil.rmtree(CONFIG[\"failure_dir\"])\n",
    "os.makedirs(CONFIG[\"failure_dir\"], exist_ok=True)\n",
    "\n",
    "\n",
    "# Load Trained Models\n",
    "\n",
    "classifier = RobustCNN(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
    "classifier.load_state_dict(torch.load(CONFIG[\"classifier_path\"], map_location=device))\n",
    "classifier.eval()\n",
    "\n",
    "generator = Generator(\n",
    "    latent_dim=CONFIG[\"latent_dim\"],\n",
    "    num_classes=CONFIG[\"num_classes\"]\n",
    ").to(device)\n",
    "generator.load_state_dict(torch.load(CONFIG[\"generator_path\"], map_location=device))\n",
    "generator.eval()\n",
    "\n",
    "print(\"‚úÖ Models loaded for failure mining\")\n",
    "\n",
    "\n",
    "#  Mine Failure Cases\n",
    "\n",
    "failure_count = 0\n",
    "\n",
    "print(f\"\\nüîç Mining {CONFIG['num_gan_samples']} GAN samples for failures...\")\n",
    "print(f\"   Confidence threshold: {CONFIG['confidence_threshold']}\")\n",
    "\n",
    "for i in tqdm(range(CONFIG[\"num_gan_samples\"]), desc=\"Mining\"):\n",
    "    z = torch.randn(1, CONFIG[\"latent_dim\"], device=device)\n",
    "    label = torch.randint(0, CONFIG[\"num_classes\"], (1,), device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_img = generator(z, label)\n",
    "        logits = classifier(fake_img)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        max_confidence, pred_class = probs.max(dim=1)\n",
    "\n",
    "    \n",
    "    if max_confidence.item() < CONFIG[\"confidence_threshold\"]:\n",
    "        failure_count += 1\n",
    "        save_image(\n",
    "            fake_img,\n",
    "            os.path.join(CONFIG[\"failure_dir\"], f\"failure_{failure_count}.png\"),\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "print(f\"\\nFailure Mining Complete!\")\n",
    "print(f\"   Total failures found: {failure_count}\")\n",
    "print(f\"   Saved to: {os.path.abspath(CONFIG['failure_dir'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b7b96",
   "metadata": {},
   "source": [
    "\n",
    "#  Phase IV: Retrain Classifier (Hardening)\n",
    "\n",
    "Create a mixed dataset of real images + failure cases, then fine-tune the classifier to become more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d05193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading training data from CSV (correct labels)...\n",
      "   Loaded 39209 samples from Train.csv\n",
      "   Label range: 0 to 42\n",
      "\n",
      "üìä Selecting 10000 random real images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving real images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:04<00:00, 2009.48it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saved 10000 real images with CORRECT labels\n",
      "‚úÖ Loaded classifier for pseudo-labeling\n",
      "\n",
      "üìä Pseudo-labeling 699 failure cases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling failures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 699/699 [00:02<00:00, 347.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Mixed dataset prepared!\n",
      "   Real images: 10000\n",
      "   Failure images: 699\n",
      "\n",
      "üîç Verifying label distribution in real images...\n",
      "   Sample of labels found: {0: 1, 1: 3, 2: 6, 3: 4, 4: 4, 5: 8, 6: 1, 7: 2, 8: 4, 9: 6}\n",
      "   Total unique labels: 34\n",
      "‚úÖ Label distribution looks good!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  PHASE IV-A: PREPARE MIXED DATASET\n",
    "\n",
    "if os.path.exists(CONFIG[\"retrain_data_dir\"]):\n",
    "    shutil.rmtree(CONFIG[\"retrain_data_dir\"])\n",
    "\n",
    "real_dir = os.path.join(CONFIG[\"retrain_data_dir\"], \"real\")\n",
    "fail_dir = os.path.join(CONFIG[\"retrain_data_dir\"], \"failure\")\n",
    "os.makedirs(real_dir, exist_ok=True)\n",
    "os.makedirs(fail_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# üîπ Load GTSRB Training Data (CSV-BASED - CORRECT LABELS!)\n",
    "\n",
    "print(\" Loading training data from CSV (correct labels)...\")\n",
    "gtsrb_train = GTSRBDataset(\n",
    "    csv_file=CONFIG[\"train_csv\"],\n",
    "    root_dir=CONFIG[\"data_root\"],\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "#  Save RANDOM Real Images (prevents class bias)\n",
    "\n",
    "print(f\"\\nSelecting {CONFIG['num_real_samples']} random real images...\")\n",
    "\n",
    "indices = torch.randperm(len(gtsrb_train))[:CONFIG[\"num_real_samples\"]]\n",
    "\n",
    "for i, idx in enumerate(tqdm(indices, desc=\"Saving real images\")):\n",
    "    \n",
    "    img, label = gtsrb_train[idx.item()]\n",
    "    save_image(\n",
    "        img,\n",
    "        os.path.join(real_dir, f\"real_{i}_label_{label}.png\"),\n",
    "        normalize=True\n",
    "    )\n",
    "\n",
    "print(f\"   Saved {len(indices)} real images with CORRECT labels\")\n",
    "\n",
    "\n",
    "# Load Classifier for Pseudo-labeling\n",
    "   \n",
    "classifier = RobustCNN(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
    "classifier.load_state_dict(torch.load(CONFIG[\"classifier_path\"], map_location=device))\n",
    "classifier.eval()\n",
    "print(f\"‚úÖ Loaded classifier for pseudo-labeling\")\n",
    "\n",
    "\n",
    "#  Pseudo-label Failure Images\n",
    "\n",
    "failure_files = [f for f in os.listdir(CONFIG[\"failure_dir\"]) if f.endswith(\".png\")]\n",
    "print(f\"\\nPseudo-labeling {len(failure_files)} failure cases...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, file in enumerate(tqdm(failure_files, desc=\"Labeling failures\")):\n",
    "        try:\n",
    "            img = Image.open(os.path.join(CONFIG[\"failure_dir\"], file)).convert(\"RGB\")\n",
    "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            logits = classifier(img_tensor)\n",
    "            pseudo_label = logits.argmax(dim=1).item()\n",
    "            \n",
    "            save_image(\n",
    "                img_tensor.squeeze(0),\n",
    "                os.path.join(fail_dir, f\"failure_{idx}_label_{pseudo_label}.png\"),\n",
    "                normalize=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {file}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Mixed dataset prepared!\")\n",
    "print(f\"   Real images: {len(os.listdir(real_dir))}\")\n",
    "print(f\"   Failure images: {len(os.listdir(fail_dir))}\")\n",
    "\n",
    "\n",
    "# VERIFY: Check label distribution\n",
    "\n",
    "print(\"\\n Verifying label distribution in real images...\")\n",
    "label_counts = {}\n",
    "for fname in os.listdir(real_dir)[:100]:\n",
    "    if \"label_\" in fname:\n",
    "        label = int(fname.split(\"label_\")[1].split(\".\")[0])\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "print(f\"   Sample of labels found: {dict(list(sorted(label_counts.items()))[:10])}\")\n",
    "print(f\"   Total unique labels: {len(label_counts)}\")\n",
    "\n",
    "if len(label_counts) < 10:\n",
    "    print(\" WARNING: Very few unique labels! Something may be wrong.\")\n",
    "else:\n",
    "    print(\"Label distribution looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1d55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mixed dataset loaded: 10699 samples\n",
      "‚úÖ Loaded base classifier from ./models/classifier.pth\n",
      "\n",
      "üõ°Ô∏è Starting hardening (fine-tuning) for 5 epochs...\n",
      "   Learning rate: 1e-05 (very low to preserve knowledge)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrain Epoch 1/5:   0%|          | 0/168 [00:00<?, ?it/s]/Users/ayushgourav/projectss/AgenticGAN---Tester-/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Retrain Epoch 1/5:   0%|          | 0/168 [00:00<?, ?it/s, loss=0.0759]/Users/ayushgourav/projectss/AgenticGAN---Tester-/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Retrain Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:06<00:00, 24.87it/s, loss=0.0118]\n",
      "Retrain Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:06<00:00, 24.87it/s, loss=0.0118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] | Loss: 15.3783 | Accuracy: 96.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrain Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:05<00:00, 29.40it/s, loss=0.0059]\n",
      "Retrain Epoch 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:05<00:00, 29.40it/s, loss=0.0059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] | Loss: 12.1776 | Accuracy: 97.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrain Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:05<00:00, 29.95it/s, loss=0.0000]\n",
      "Retrain Epoch 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:05<00:00, 29.95it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] | Loss: 10.5980 | Accuracy: 97.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrain Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:05<00:00, 30.01it/s, loss=0.0216]\n",
      "Retrain Epoch 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:05<00:00, 30.01it/s, loss=0.0216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] | Loss: 10.5717 | Accuracy: 98.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrain Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:08<00:00, 20.75it/s, loss=0.0000]\n",
      "Retrain Epoch 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168/168 [00:08<00:00, 20.75it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] | Loss: 8.8608 | Accuracy: 98.17%\n",
      "\n",
      "‚úÖ Hardened classifier saved to: /Users/ayushgourav/projectss/AgenticGAN---Tester-/GTSRBT/AGENT/models/classifier_hardened.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  PHASE IV-B: LOAD MIXED DATASET & RETRAIN\n",
    "# \n",
    "\n",
    "\n",
    "#  Create Mixed DataLoader\n",
    "\n",
    "real_dir = os.path.join(CONFIG[\"retrain_data_dir\"], \"real\")\n",
    "fail_dir = os.path.join(CONFIG[\"retrain_data_dir\"], \"failure\")\n",
    "\n",
    "mixed_dataset = MixedDataset(real_dir, fail_dir, transform=transform)\n",
    "mixed_loader = DataLoader(\n",
    "    mixed_dataset,\n",
    "    batch_size=CONFIG[\"classifier_batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],  \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖMixed dataset loaded: {len(mixed_dataset)} samples\")\n",
    "\n",
    "\n",
    "#  Load BASE Classifier (not random!)\n",
    "\n",
    "classifier = RobustCNN(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
    "classifier.load_state_dict(torch.load(CONFIG[\"classifier_path\"], map_location=device))\n",
    "print(f\"Loaded base classifier from {CONFIG['classifier_path']}\")\n",
    "\n",
    "\n",
    "#  Fine-tuning Setup (VERY LOW learning rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=CONFIG[\"retrain_lr\"])\n",
    "\n",
    "\n",
    "# Fine-tuning Loop\n",
    "\n",
    "print(f\"\\nStarting hardening (fine-tuning) for {CONFIG['retrain_epochs']} epochs...\")\n",
    "print(f\"   Learning rate: {CONFIG['retrain_lr']} (very low to preserve knowledge)\")\n",
    "\n",
    "classifier.train()\n",
    "\n",
    "for epoch in range(CONFIG[\"retrain_epochs\"]):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(mixed_loader, desc=f\"Retrain Epoch {epoch+1}/{CONFIG['retrain_epochs']}\")\n",
    "    for images, labels in loop:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{CONFIG['retrain_epochs']}] | Loss: {running_loss:.4f} | Accuracy: {acc:.2f}%\")\n",
    "    \n",
    "    if acc < 50.0:\n",
    "        print(\"‚ö†Ô∏è WARNING: Accuracy is low. Check if mixed dataset has correct labels.\")\n",
    "\n",
    "torch.save(classifier.state_dict(), CONFIG[\"hardened_path\"])\n",
    "print(f\"\\nHardened classifier saved to: {os.path.abspath(CONFIG['hardened_path'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5bca5",
   "metadata": {},
   "source": [
    "---\n",
    "# Phase V: Evaluation & Visualization\n",
    "\n",
    "Compare accuracy before and after hardening, and visualize with Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f392ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading test data...\n",
      "   Loaded 12630 samples from Test.csv\n",
      "   Label range: 0 to 42\n",
      "‚úÖ Loaded base model from ./models/classifier.pth\n",
      "‚úÖ Loaded hardened model from ./models/classifier_hardened.pth\n",
      "\n",
      "üìä Evaluating Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base Model:   0%|          | 0/198 [00:00<?, ?it/s]/Users/ayushgourav/projectss/AgenticGAN---Tester-/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Base Model:   2%|‚ñè         | 4/198 [00:00<00:05, 34.79it/s]/Users/ayushgourav/projectss/AgenticGAN---Tester-/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Base Model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:05<00:00, 37.48it/s]\n",
      "Base Model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:05<00:00, 37.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating Hardened Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardened Model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:05<00:00, 38.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä RESULTS SUMMARY\n",
      "==================================================\n",
      "‚úÖ Accuracy BEFORE Hardening: 96.17%\n",
      "‚úÖ Accuracy AFTER Hardening:  96.73%\n",
      "üìà Improvement: +0.56%\n",
      "==================================================\n",
      "\n",
      "üìÑ Results saved to: /Users/ayushgourav/projectss/AgenticGAN---Tester-/GTSRBT/AGENT/results/accuracy_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  PHASE V-A: EVALUATE ACCURACY (BEFORE VS AFTER)\n",
    "\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_data = GTSRBDataset(\n",
    "    csv_file=CONFIG[\"test_csv\"],\n",
    "    root_dir=CONFIG[\"data_root\"],\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=CONFIG[\"classifier_batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=desc):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "\n",
    "base_model = RobustCNN(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
    "if os.path.exists(CONFIG[\"classifier_path\"]):\n",
    "    base_model.load_state_dict(torch.load(CONFIG[\"classifier_path\"], map_location=device))\n",
    "    print(f\"Loaded base model from {CONFIG['classifier_path']}\")\n",
    "else:\n",
    "    print(\"Base model not found!\")\n",
    "\n",
    "\n",
    "hardened_model = RobustCNN(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
    "if os.path.exists(CONFIG[\"hardened_path\"]):\n",
    "    hardened_model.load_state_dict(torch.load(CONFIG[\"hardened_path\"], map_location=device))\n",
    "    print(f\"Loaded hardened model from {CONFIG['hardened_path']}\")\n",
    "else:\n",
    "    print(\"Hardened model not found!\")\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating Base Model...\")\n",
    "acc_before = evaluate_model(base_model, test_loader, \"Base Model\")\n",
    "\n",
    "print(\"\\nEvaluating Hardened Model...\")\n",
    "acc_after = evaluate_model(hardened_model, test_loader, \"Hardened Model\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Accuracy BEFORE Hardening: {acc_before:.2f}%\")\n",
    "print(f\" Accuracy AFTER Hardening:  {acc_after:.2f}%\")\n",
    "print(f\"Improvement: {acc_after - acc_before:+.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "results_file = os.path.join(CONFIG[\"results_dir\"], \"accuracy_results.txt\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(\"AgenticGAN GTSRB Results\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    f.write(f\"Accuracy Before Hardening: {acc_before:.2f}%\\n\")\n",
    "    f.write(f\"Accuracy After Hardening:  {acc_after:.2f}%\\n\")\n",
    "    f.write(f\"Improvement: {acc_after - acc_before:+.2f}%\\n\")\n",
    "\n",
    "print(f\"\\nüìÑ Results saved to: {os.path.abspath(results_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e16aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Grad-CAM saved to: /Users/ayushgourav/projectss/AgenticGAN---Tester-/GTSRBT/AGENT/results/gradcam/gradcam_sample.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#PHASE V-B: GRAD-CAM VISUALIZATION\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# üîπ Grad-CAM Class\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "\n",
    "        # Register hooks\n",
    "        self.target_layer.register_forward_hook(self._forward_hook)\n",
    "        self.target_layer.register_full_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def generate(self, image, class_idx=None):\n",
    "        self.model.eval()\n",
    "        image = image.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = self.model(image)\n",
    "\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1).item()\n",
    "\n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        output[0, class_idx].backward()\n",
    "\n",
    "        # Compute Grad-CAM\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # Normalize\n",
    "        cam -= cam.min()\n",
    "        cam /= (cam.max() + 1e-8)\n",
    "\n",
    "        return cam.squeeze().cpu().numpy()\n",
    "\n",
    "target_layer = hardened_model.features[10]\n",
    "gradcam = GradCAM(hardened_model, target_layer)\n",
    "\n",
    "# Get a test image\n",
    "img, label = test_data[0]\n",
    "input_tensor = img.unsqueeze(0).to(device)\n",
    "\n",
    "# Generate CAM\n",
    "cam = gradcam.generate(input_tensor)\n",
    "\n",
    "# Create heatmap overlay\n",
    "cam_resized = cv2.resize(cam, (32, 32))\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "\n",
    "# Denormalize image\n",
    "img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "img_np = (img_np * 0.5 + 0.5)  # back to [0,1]\n",
    "img_np = np.uint8(255 * img_np)\n",
    "\n",
    "# Overlay\n",
    "overlay = cv2.addWeighted(img_np, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "# Save\n",
    "gradcam_dir = os.path.join(CONFIG[\"results_dir\"], \"gradcam\")\n",
    "os.makedirs(gradcam_dir, exist_ok=True)\n",
    "save_path = os.path.join(gradcam_dir, \"gradcam_sample.png\")\n",
    "cv2.imwrite(save_path, overlay)\n",
    "\n",
    "print(f\"‚úÖ Grad-CAM saved to: {os.path.abspath(save_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36698c14",
   "metadata": {},
   "source": [
    "\n",
    "##  Output Files:\n",
    "- **Models:** `./models/`\n",
    "  - `classifier.pth` - Base classifier\n",
    "  - `gan_generator.pth` - Trained generator\n",
    "  - `gan_discriminator.pth` - Trained discriminator\n",
    "  - `classifier_hardened.pth` - Hardened classifier\n",
    "\n",
    "- **Generated Images:** `./generated_images/`\n",
    "- **Failure Cases:** `./failure_cases/`\n",
    "- **Results:** `./results/`\n",
    "  - `accuracy_results.txt`\n",
    "  - `gradcam/gradcam_sample.png`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
